--------------------------------------------------
PCA
--------------------------------------------------

from numpy import array,mean,cov
from numpy.linalg import eig
A = array([[90,60,90], [90,90,30], [60,60,60],[60,60,90],[30,30,30]])
M = mean(A.T, axis=1)
C = A-M
V = cov(C.T)
values , vectors = eig(V)
total = sum(values)
for i in values:
  percentage = (i*100)/total
  print( i,' => ', percentage,'%')

-----------------------------
SVD
-----------------------------

from numpy import array,dot,multiply
from numpy.linalg import eig,det,inv
from math import sqrt

A = array([[3,3,2],[2,3,2],[1,2,3]])
print(A)

A_trans = A.T
print(A_trans)

A_A_trans = dot(A,A_trans)
print(A_A_trans)

A_trans_A = array(dot(A_trans, A))
print(A_trans_A)

value, vector = eig(A_A_trans)
print(value)
print(vector)

singular = []
for i in value:
  singular.append(sqrt(i))

singular.sort(reverse=True)

s = array([
            [singular[0],0,0],
            [0,singular[1],0],
            [0,0,singular[2]]
           ])
print(s)

s_inverse = inv(s)
print(s_inverse)

val , vec = eig(A_trans_A)
V = vec.T
print(V)

U = dot(dot(A,V),s_inverse)
print(U)

ans = dot(dot(U,s),vec)
print(ans)

-------------------------------
Decision tree
-------------------------------

import pandas as pd
import numpy as np

dataset=pd.read_csv('User_Data.csv')

x=dataset[['Age','EstimatedSalary']].values

y=dataset[['Purchased']].values

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)

from sklearn.preprocessing import StandardScaler

st_x = StandardScaler()

x_train=st_x.fit_transform(x_train)

x_test=st_x.transform(x_test)

from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier(criterion='entropy',random_state=0)

classifier.fit(x_train,y_train)

y_pred=classifier.predict(x_test)

from sklearn.metrics import confusion_matrix
x_mat = confusion_matrix(y_test,y_pred)


---------------------------------
KNN
---------------------------------

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import pandas as pd

irisData = pd.read_csv('iris.csv')

X = irisData[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]
y = irisData[['Species']]

X_train, X_test, y_train, y_test = train_test_split(
			X, y, test_size = 0.2)

knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train, y_train)

print(knn.predict(X_test))

-------------------------------------------
Linear regression
-------------------------------------------

import pandas as pd

from sklearn.linear_model import LinearRegression

dataset=pd.read_csv('iris.csv')

x=dataset[['SepalLengthCm']]
y=dataset[['SepalWidthCm']]

lr=LinearRegression()

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)

lr.fit(x_train,y_train)

y_pred=lr.predict(x_test)

from sklearn.metrics import mean_squared_error

mean_squared_error(y_test,y_pred)

import matplotlib.pyplot as plt
plt.scatter(x_test, y_test, color ='b')
plt.plot(x_test, y_pred, color ='k')
  
plt.show()


-------------Multiple Regression---------------------
 
import numpy as np
import pandas as pd
 
dataset = pd.read_csv('50_Startups.csv')
dataset.head()
 
X = dataset[['R&D Spend','Administration','Marketing Spend','State']]
Y = dataset['Profit']
 
States = pd.get_dummies(X['State'],drop_first=True)
X = X.drop('State',axis=1)
X = pd.concat([X,States],axis=1)
 
X.head()
 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)
 
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)
 
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

----------------
Naive bayes 
----------------

import numpy as np

import pandas as pd

from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

play_tennis = pd.read_csv('PlayTennis.csv')

number=LabelEncoder()

play_tennis['Outlook']=number.fit_transform(play_tennis['Outlook'])

play_tennis['Temperature']=number.fit_transform(play_tennis['Temperature'])

play_tennis['Humidity']=number.fit_transform(play_tennis['Humidity'])

play_tennis['Wind']=number.fit_transform(play_tennis['Wind'])

play_tennis['Play Tennis']=number.fit_transform(play_tennis['Play Tennis'])

features=['Outlook','Temperature','Humidity','Wind']

target="Play Tennis"

x_train,x_test,y_train,y_test = train_test_split(play_tennis[features],play_tennis[target])

model=GaussianNB()

model.fit(x_train,y_train)

pred=model.predict(x_test)

accuracy =accuracy_score(y_test,pred)


-----------------------
Tree Classifier
-----------------------

import pandas as pd
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score
from sklearn.preprocessing import LabelEncoder


data = pd.read_csv('car_evaluation.csv')
data.head()

lb=LabelEncoder()data['vhigh']=lb.fit_transform(data['vhigh']);
data['small']=lb.fit_transform(data['small']);

data['low']=lb.fit_transform(data['low']);
data['unacc']=lb.fit_transform(data['unacc']);

X = data[['vhigh','small','low']]
y=data['unacc']

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=41)

dtree=DecisionTreeClassifier()
dtree.fit(X_train,y_train)
y_pred = dtree.predict(X_test)
accuracy_score(y_test,y_pred)

import matplotlib.pyplot as plt
plt.figure(figsize=(20,20))
dec_tree =plot_tree(decision_tree=dtree,feature_names=data.columns,filled=True,rounded=True)
plt.savefig("one.png")



------------------C-4.5------------------------

pip install chefboost
from chefboost import Chefboost as chef
import pandas as pd

df = pd.read_csv("Golf.csv")
df.head()

config = {'algorithm': 'C4.5'}
model = chef.fit(df, config = config, target_label = 'Decision')

chef.save_model(model, "model.pkl")
model = chef.load_model("model.pkl")

#Checking With Actual Dataset
print('Predict  -    Actual')
print('"""""""""""""""""""""')
for index,instance in df.iterrows():
  predict = chef.predict(model, instance)
  actual = instance['Decision']
  print(predict, '\t -\t',actual)

# Predict Any Upcoming Entry
prediction = chef.predict(model, ['Overcast',85,85,'Weak'])
print(prediction)


---------------------------------------
Decision Tree CART
---------------------------------------

# CART on the Bank Note dataset
from random import seed
from random import randrange
from csv import reader

# Load a CSV file
def load_csv(filename):
	file = open(filename, "rb")
	lines = reader(file)
	dataset = list(lines)
	return dataset

# Convert string column to float
def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())

# Split a dataset into k folds
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split

# Calculate accuracy percentage
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0

# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores

# Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] < value:
			left.append(row)
		else:
			right.append(row)
	return left, right

# Calculate the Gini index for a split dataset
def gini_index(groups, classes):
	# count all samples at split point
	n_instances = float(sum([len(group) for group in groups]))
	# sum weighted Gini index for each group
	gini = 0.0
	for group in groups:
		size = float(len(group))
		# avoid divide by zero
		if size == 0:
			continue
		score = 0.0
		# score the group based on the score for each class
		for class_val in classes:
			p = [row[-1] for row in group].count(class_val) / size
			score += p * p
		# weight the group score by its relative size
		gini += (1.0 - score) * (size / n_instances)
	return gini

# Select the best split point for a dataset
def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini < b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}

# Create a terminal node value
def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)

# Create child splits for a node or make terminal
def split(node, max_depth, min_size, depth):
	left, right = node['groups']
	del(node['groups'])
	# check for a no split
	if not left or not right:
		node['left'] = node['right'] = to_terminal(left + right)
		return
	# check for max depth
	if depth >= max_depth:
		node['left'], node['right'] = to_terminal(left), to_terminal(right)
		return
	# process left child
	if len(left) <= min_size:
		node['left'] = to_terminal(left)
	else:
		node['left'] = get_split(left)
		split(node['left'], max_depth, min_size, depth+1)
	# process right child
	if len(right) <= min_size:
		node['right'] = to_terminal(right)
	else:
		node['right'] = get_split(right)
		split(node['right'], max_depth, min_size, depth+1)

# Build a decision tree
def build_tree(train, max_depth, min_size):
	root = get_split(train)
	split(root, max_depth, min_size, 1)
	return root

# Make a prediction with a decision tree
def predict(node, row):
	if row[node['index']] < node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']

# Classification and Regression Tree Algorithm
def decision_tree(train, test, max_depth, min_size):
	tree = build_tree(train, max_depth, min_size)
	predictions = list()
	for row in test:
		prediction = predict(tree, row)
		predictions.append(prediction)
	return(predictions)

# Test CART on Bank Note dataset
seed(1)
# load and prepare data
filename = 'data_banknote_authentication.csv'
dataset = load_csv(filename)
# convert string attributes to integers
for i in range(len(dataset[0])):
	str_column_to_float(dataset, i)
# evaluate algorithm
n_folds = 5
max_depth = 5
min_size = 10
scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)
print('Scores: %s' % scores)
print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))